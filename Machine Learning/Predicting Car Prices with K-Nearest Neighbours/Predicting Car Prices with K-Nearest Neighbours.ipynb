{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Applying K-Nearest Neighbors: Predicting Car Prices</h1>\n",
    "\n",
    "In the DataQuest Machine Learning course, I explored the fundamentals of machine learning using the k-nearest neighbors algorithm. In this project, I'll practice the machine learning workflow I've learned so far to predict a car's market price using its attributes. The data set I will be working with contains information on various cars. For each car I have information about the technical aspects of the vehicle such as the motor's displacement, the weight of the car, the miles per gallon, how fast the car accelerates, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('imports-85.data')\n",
    "cars.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the data is a little messy. The column names do not match up with the ones in the original data. I will define the column names myself and match them up by passing them into the read_csv call under the names parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', \n",
    "        'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', \n",
    "        'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-rate', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
    "cars = pd.read_csv('imports-85.data', names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my predictive model I only want feature columns that contain numeric and orginal values. Below I have selected columns that fit this criteria ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_values_cols = ['normalized-losses', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'bore', 'stroke', 'compression-rate', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
    "car_features = cars[ordinal_values_cols]\n",
    "car_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a predictive model, it is important to not have any missing values. Based on the data set preview from the last step, I can tell that the normalized-losses column contains missing values represented using \"?\". Let's replace these values and look for the presence of missing values in other numeric columns. Let's also normalize the values in all numeric columns so they have a value between zero and one. That way very large values will not have a greater influence, and everything will be relative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace all ? values with NaN values\n",
    "car_features.replace('?', np.nan, inplace=True)\n",
    "#Make sure all values are type float\n",
    "car_features = car_features.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets detemine how many rows have missing values and in which columns these missing values exist. If it is a significant quantity, I might need to drop the column, otherwise I will just replace missing values with the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_features.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price column contains our label, and so missing data is a big problem here. I will remove these rows to fix this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_features = car_features.dropna(subset=['price'])\n",
    "car_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those rows disposed off, I will replace any remaining null values with the mean value of their respective column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "car_features = car_features.fillna(car_features.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! No more missing data! Now it is time to normalise the data so that they all scale between 0 and 1. The only column I don't want to normalise it the price, as this is the label I will be using for my predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_price = car_features['price']\n",
    "normalised_data = (car_features.max() - car_features)/(car_features.max())\n",
    "normalised_data['price'] = car_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Univariant K-nearest Neighbours Model</h2>\n",
    "\n",
    "I will start with a simple univariant model thats making predictions based on a single feature. Starting with simple models before moving to more complex models helps us structure your code workflow and understand the features better.\n",
    "For now we will use test/train validation, and then later we will try cross validation. So in the function below the data will be split evenly into test/train.\n",
    "\n",
    "There are two parameters we can vary with K-Nearest Neighbours, the k value (the number of neighbours) and the amount of features we are using for our comparison.\n",
    "\n",
    "In the function below I will vary the k value, and iterate through each numerical column in the car data. Then I will plot the Root Mean Squared Error for each k value, for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_train_test(df, feature_columns, label_columns,k_range):\n",
    "    \"\"\"Instantiate a K-nearest neighbours model and fit\n",
    "    with data provided\n",
    "    Keyword Arguments:\n",
    "    df -- Pandas dataframe\n",
    "    feature_columns -- columns containing feature elements\n",
    "    label_columns -- columns containing labels\n",
    "    k_range -- k values for knn parameters\"\"\"\n",
    "    rmse_values = {}\n",
    "    #Randomise the dataset\n",
    "    np.random.seed(1)\n",
    "    df = df.reindex(np.random.permutation(df.index))\n",
    "    #Split the data evenly into test set and train set\n",
    "    split_idx = int(df.shape[0]/2)\n",
    "    train = df.iloc[0:split_idx]\n",
    "    test = df.iloc[split_idx:]\n",
    "    for k in k_range:\n",
    "        #Instantiate the KNeighborsRegressor class\n",
    "        knn = KNeighborsRegressor(n_neighbors=k)\n",
    "        #Fit the model with our data\n",
    "        knn.fit(train[feature_columns], train[label_columns])\n",
    "        #Make predictions using the test features\n",
    "        predictions = knn.predict(test[feature_columns])\n",
    "        #Calculate the mean squared error of our prediction\n",
    "        mse = mean_squared_error(test[label_columns], predictions)\n",
    "        #Return the RMSE by taking the square root of the MSE\n",
    "        rmse_values[k] = (np.sqrt(mse))\n",
    "    return rmse_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = normalised_data.columns.drop('price')\n",
    "column_rmse_values = {}\n",
    "for col in feature_columns:\n",
    "    column_rmse_values[col] = knn_train_test(normalised_data,\n",
    "                                            [col], 'price', \n",
    "                                            [1,3,5,7,9])\n",
    "column_rmse_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "for k,v in column_rmse_values.items():\n",
    "    ax = plt.subplot(111)\n",
    "    lists = sorted(column_rmse_values[k].items())\n",
    "    x, y = zip(*lists)\n",
    "    ax.plot(x,y, label = k)\n",
    "ax.set_xlabel('k value')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that an optimum k value exists above 3, and the safest bet would be around a value of 5. The columns with the lowest error are 'Horsepower', 'width', 'curb-weight', 'city-mpg', 'highway-mpg', and 'length'. This is possibly suggesting how well known measures of performance, miles per gallon and horsepower, have a powerful influence over price. The other attribute is the size of the vehicle. \n",
    "\n",
    "I will now pass in multiple collumns as feature variables to the k-nearest neighbour function I defined earlier, and will therefore be creating a multi-variant model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use a k value of 5\n",
    "k = 5\n",
    "rmse_results = {}\n",
    "two_best_features = ['horsepower', 'width']\n",
    "rmse = knn_train_test(normalised_data, two_best_features, 'price',[k])\n",
    "#The function returns a dictionary, because there is the option\n",
    "#to use multiple k values. We just want the one result for k = 5\n",
    "rmse_results[2] = rmse[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now increase the number of features and then plot the results to find the optimum number of features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_best_features = ['horsepower', 'width', 'curb-weight']\n",
    "four_best_features = ['horsepower', 'width', 'curb-weight', 'city-mpg']\n",
    "five_best_features = ['horsepower', 'width', 'curb-weight' , 'city-mpg' , 'highway-mpg']\n",
    "six_best_features = ['horsepower', 'width', 'curb-weight' , 'city-mpg' , 'highway-mpg', 'length']\n",
    "rmse = knn_train_test(normalised_data, three_best_features, 'price',[k])\n",
    "rmse_results[3] = rmse[5]\n",
    "rmse = knn_train_test(normalised_data, four_best_features, 'price',[k])\n",
    "rmse_results[4] = rmse[5]\n",
    "rmse = knn_train_test(normalised_data, five_best_features, 'price',[k])\n",
    "rmse_results[5] = rmse[5]\n",
    "rmse = knn_train_test(normalised_data, six_best_features, 'price',[k])\n",
    "rmse_results[6] = rmse[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [v for k,v in rmse_results.items()]\n",
    "x = np.arange(2,7,1)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above it is clear that the optimum number of features is 3 or 4. I will now vary the k value between 1 and 25 for both these features and then plot the results to find the optimum model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.arange(1,26,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse_values_3 = knn_train_test(normalised_data, three_best_features, 'price',k)\n",
    "rmse_values_4 = knn_train_test(normalised_data, four_best_features, 'price',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.subplot(111)\n",
    "y = [v for k,v in rmse_values_3.items()]\n",
    "x = k\n",
    "ax.plot(x,y, label = 'Three Features')\n",
    "y = [v for k,v in rmse_values_4.items()]\n",
    "x = k\n",
    "ax.plot(x,y, label = 'Four Features')\n",
    "ax.set_xlabel('k value')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal k value when using three features is 5, whereas the optimal value when using four features is 2.\n",
    "The difference between the k values is on account of the addition of 'city-mpg' to the prediction model. \n",
    "The best performance comes from the model that takes four features and uses a k value of 2. \n",
    "\n",
    "<h2>K-Fold Cross Validation</h2>\n",
    "\n",
    "Up until now I have been using train/test validation. In train/test validation the model is repeatedly biased towards a specific subset of the data. K-fold cross validation on the other hand, takes advantage of a larger proportion of the dataset during training, whilst rotating through the data for test subsets, thus avoiding the issues encountered by train/test validation.\n",
    "\n",
    "The general algorithm is as follows:\n",
    "* Split the full dataset into k equal length partitions\n",
    "    * select k-1 partitions as training data\n",
    "    * select the remaining partition as the test set\n",
    "* Train the model on the training set to predict labels on the test subset\n",
    "* Compute the test fold's error metric\n",
    "* Repeat the above steps k - 1 times, until each partition has been used as the test set for an iteration\n",
    "* Calculate the mean of the k error values\n",
    "\n",
    "Generally 5 or 10 folds are used for k-folds cross validation. I will be using 5 folds in my example below.\n",
    "\n",
    "I will use the best model from above which was using four features and a k value of 2. \n",
    "\n",
    "To perform k-fold cross validation I will instantiate an instance of the Kfold class from sklearn.model_selection.\n",
    "This class returns an iterator object which I can use in conjunction with the cross_val_score() function, also from sklearn.model_selection.\n",
    "\n",
    "I will specify the parameter 'scoring' to be neg_mean_squared_error' which will return the mean_sqared error for each k fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "knn = KNeighborsRegressor(n_neighbors=2)\n",
    "mses = cross_val_score(knn,normalised_data[four_best_features],normalised_data['price'],scoring='neg_mean_squared_error',cv=kf)\n",
    "avg_rmses = np.mean([(np.abs(mse)**(1/2)) for mse in mses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rmses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly the RMSE was higher than observed when using test/train validation. This might demonstrate that some bias was being made towards the fixed training set I was using.\n",
    "Lets use cross validation to see what the optimum k value is when using the four feature model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_vals = np.arange(1,26,1)\n",
    "avg_rmses= {}\n",
    "for k in k_vals:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    mses = cross_val_score(knn,normalised_data[four_best_features],normalised_data['price'],scoring='neg_mean_squared_error',cv=kf)\n",
    "    avg_rmses[k] = np.mean([(np.abs(mse)**(1/2)) for mse in mses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.subplot(111)\n",
    "y = [v for k,v in avg_rmses.items()]\n",
    "x = k_vals\n",
    "ax.set_xlabel('k value')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A k-value of 2 is still optimal, so we shall remain with this k-value. I shall now use cross-validation to find the best number of features to use, ranging from three to eight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_best_features = ['horsepower', 'width', 'curb-weight' , 'city-mpg' , 'highway-mpg', 'length', 'wheel-base']\n",
    "eight_best_features = ['horsepower', 'width', 'curb-weight' , 'city-mpg' , 'highway-mpg', 'length', 'wheel-base','bore']\n",
    "knn = KNeighborsRegressor(n_neighbors=2)\n",
    "avg_rmses= {}\n",
    "features = [three_best_features, four_best_features, five_best_features,six_best_features,seven_best_features,eight_best_features]\n",
    "idx = 3\n",
    "for f in features:    \n",
    "    mses = cross_val_score(knn,normalised_data[f],normalised_data['price'],scoring='neg_mean_squared_error',cv=kf)\n",
    "    avg_rmses[idx] = np.mean([(np.abs(mse)**(1/2)) for mse in mses])\n",
    "    idx += 1\n",
    "avg_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.subplot(111)\n",
    "y = [v for k,v in avg_rmses.items()]\n",
    "x = np.arange(3,9,1)\n",
    "ax.plot(x,y)\n",
    "ax.set_xlabel('Number of Features')\n",
    "ax.set_ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in conclusion, by using cross-validation instead of test/train, we can conclude that the optimal k value is indeed 2, but by using 7 features instead of 4, we can achieve a much better RMSE.\n",
    "\n",
    "It should bbe noted however that the RMSE value does not give the whole picture, and a low RMSE is not always suggestive of an accurate model. A model has two sources of error, bias and variance.\n",
    "\n",
    "Bias is the result of bad assumptions, for example the assumption that a certain car feature relates to price. Variance, is error that occurs due to the variability of a model's predictions. If our dataset contained 1000 features, and we used all of them to train an incredibly complicated multivariant model, we will have very low bias but very high variance.\n",
    "\n",
    "In an ideal world, we want low bias but also low variance, but in reality it is a trade-off between the two.\n",
    "\n",
    "The average RMSE is a good measure of a model's bias, which is what we have above. To measure variance, we can use the standard deviation of the RMSE values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_best_features = ['horsepower', 'width', 'curb-weight' , 'city-mpg' , 'highway-mpg', 'length', 'wheel-base']\n",
    "eight_best_features = ['horsepower', 'width', 'curb-weight' , 'city-mpg' , 'highway-mpg', 'length', 'wheel-base','bore']\n",
    "knn = KNeighborsRegressor(n_neighbors=2)\n",
    "std_rmses = {}\n",
    "avg_rmses= {}\n",
    "features = [three_best_features, four_best_features, five_best_features,six_best_features,seven_best_features,eight_best_features]\n",
    "idx = 3\n",
    "for f in features:    \n",
    "    mses = cross_val_score(knn,normalised_data[f],normalised_data['price'],scoring='neg_mean_squared_error',cv=kf)\n",
    "    avg_rmses[idx] = np.mean([(np.abs(mse)**(1/2)) for mse in mses])\n",
    "    std_rmses[idx] = np.std([(np.abs(mse)**(1/2)) for mse in mses])\n",
    "    idx += 1\n",
    "print(avg_rmses)\n",
    "print(std_rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.subplot(111)\n",
    "y = [v for k,v in avg_rmses.items()]\n",
    "x = np.arange(3,9,1)\n",
    "ax.plot(x,y, label='Avg RMSE')\n",
    "y = [v for k,v in std_rmses.items()]\n",
    "x = np.arange(3,9,1)\n",
    "ax.plot(x,y, label='SD RMSE')\n",
    "\n",
    "ax.set_xlabel('Number of Features')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My conclusions remain very much the same, but I hope this has demonstrated how K-Nearest Neighbors can be applied to make predictions, and how hyperparameters can be adjusted, whilst investigating results with the correct validation methods, to produce the optimal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
